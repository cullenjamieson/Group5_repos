{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8awFvthKz1k",
        "outputId": "ebac97c0-0a42-4f57-bbaa-c9f5bcb4489e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display, clear_output\n",
        "import numpy as np\n",
        "%matplotlib nbagg\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "from torch.nn.functional import relu\n",
        "from torch.nn.functional import softmax\n",
        "import PIL.Image\n",
        "import os\n",
        "import torchvision\n",
        "import cv2\n",
        "\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import DataLoader, Dataset, Subset\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms.functional as TF\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qHheG0jv6oM",
        "outputId": "dba6b9b2-c4b6-45cb-e723-89623d940cb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.11.4)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.19.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.1)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.8.1.78)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (4.5.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2023.12.9)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (23.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "pip install albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ayULTtZEwito"
      },
      "outputs": [],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nmR7LP3jMgsN"
      },
      "outputs": [],
      "source": [
        "# Source: https://towardsdatascience.com/cook-your-first-u-net-in-pytorch-b3297a844cf3, visited the 16th of November 2023\n",
        "# Modifications have been made to the original clde with adding batch normalization, dropout and changing the input and output sizes\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_class):\n",
        "        super().__init__()\n",
        "\n",
        "        #Encoder\n",
        "        # Input: 1x128x128\n",
        "        self.e11 = nn.Conv2d(1, 64, kernel_size=3,padding=1)\n",
        "        self.bn11 = nn.BatchNorm2d(64) # batch normalization\n",
        "        self.e12 = nn.Conv2d(64, 64, kernel_size=3,padding=1)\n",
        "        self.bn12 = nn.BatchNorm2d(64) # batch normalization\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) #64x64x64\n",
        "\n",
        "        self.e21 = nn.Conv2d(64, 128, kernel_size=3,padding=1)\n",
        "        self.bn21 = nn.BatchNorm2d(128) # batch normalization\n",
        "        self.e22 = nn.Conv2d(128, 128, kernel_size=3,padding=1)\n",
        "        self.bn22 = nn.BatchNorm2d(128) # batch normalization\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) #32x32x128\n",
        "\n",
        "        self.e31 = nn.Conv2d(128, 256, kernel_size=3,padding=1)\n",
        "        self.bn31 = nn.BatchNorm2d(256) # batch normalization\n",
        "        self.e32 = nn.Conv2d(256, 256, kernel_size=3,padding=1)\n",
        "        self.bn32 = nn.BatchNorm2d(256) # batch normalization\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) #16x16x256\n",
        "\n",
        "        self.e41 = nn.Conv2d(256, 512, kernel_size=3,padding=1)\n",
        "        self.bn41 = nn.BatchNorm2d(512) # batch normalization\n",
        "        self.e42 = nn.Conv2d(512, 512, kernel_size=3,padding=1)\n",
        "        self.bn42 = nn.BatchNorm2d(512) # batch normalization\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) #8x8x512\n",
        "\n",
        "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3,padding=1)\n",
        "        self.bn51 = nn.BatchNorm2d(1024) # batch normalization\n",
        "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3,padding=1)\n",
        "        self.bn52 = nn.BatchNorm2d(1024) # batch normalization\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        # Decoder\n",
        "        self.upconv1 = nn.ConvTranspose2d(1024,512,kernel_size=2,stride=2) #16x16x1024\n",
        "        self.d11 = nn.Conv2d(1024,512,kernel_size=3,padding=1)\n",
        "        self.d12 = nn.Conv2d(512,512,kernel_size=3,padding=1)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(512,256,kernel_size=2,stride=2) #\n",
        "        self.d21 = nn.Conv2d(512,256,kernel_size=3,padding=1)\n",
        "        self.d22 = nn.Conv2d(256,256,kernel_size=3,padding=1)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(256,128,kernel_size=2,stride=2)\n",
        "        self.d31 = nn.Conv2d(256,128,kernel_size=3,padding=1)\n",
        "        self.d32 = nn.Conv2d(128,128,kernel_size=3,padding=1)\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(128,64,kernel_size=2,stride=2)\n",
        "        self.d41 = nn.Conv2d(128,64,kernel_size=3,padding=1)\n",
        "        self.d42 = nn.Conv2d(64,64,kernel_size=3,padding=1)\n",
        "\n",
        "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        xe11 = F.relu(self.bn11(self.e11(x)))\n",
        "        xe12 = F.relu(self.bn12(self.e12(xe11)))\n",
        "        xp1 = self.pool1(xe12)\n",
        "\n",
        "        #xp1 = self.dropout(xp1) # dropout\n",
        "        xe21 = F.relu(self.bn21(self.e21(xp1)))\n",
        "        xe22 = F.relu(self.bn22(self.e22(xe21)))\n",
        "        xp2 = self.pool2(xe22)\n",
        "\n",
        "        xp2 = self.dropout(xp2) # dropout\n",
        "        xe31 = F.relu(self.bn31(self.e31(xp2)))\n",
        "        xe32 = F.relu(self.bn32(self.e32(xe31)))\n",
        "        xp3 = self.pool3(xe32)\n",
        "\n",
        "        xp3 = self.dropout(xp3) # dropout\n",
        "        xe41 = F.relu(self.bn41(self.e41(xp3)))\n",
        "        xe42 = F.relu(self.bn42(self.e42(xe41)))\n",
        "        xp4 = self.pool4(xe42)\n",
        "\n",
        "        xp4 = self.dropout(xp4) # dropout\n",
        "        xe51 = F.relu(self.bn51(self.e51(xp4)))\n",
        "        xe52 = F.relu(self.bn52(self.e52(xe51)))\n",
        "\n",
        "        # Up-convolutions\n",
        "        xup1 = self.upconv1(xe52)\n",
        "        xup1 = self.dropout(xup1) # dropout\n",
        "        xcat = torch.cat([xup1, xe42], dim=1)\n",
        "        #xcat=xup1\n",
        "\n",
        "        xup21 = F.relu(self.d11(xcat))\n",
        "        xup22 = F.relu(self.d12(xup21))\n",
        "\n",
        "        xup2 = self.upconv2(xup22)\n",
        "        xup2 = self.dropout(xup2) # dropout\n",
        "        #xcat2 = torch.cat([xup2, xe32[:,:,:-1,:-1]], dim=1)\n",
        "        xcat2 = torch.cat([xup2, xe32], dim=1)\n",
        "        #xcat2=xup2\n",
        "\n",
        "\n",
        "        xup31 = F.relu(self.d21(xcat2))\n",
        "        xup32 = F.relu(self.d22(xup31))\n",
        "        xup3 = self.upconv3(xup32)\n",
        "        #xup3 = self.dropout(xup3) # dropout\n",
        "        xcat3 = torch.cat([xup3, xe22], dim=1)\n",
        "        #xcat3=xup3\n",
        "\n",
        "        xup41 = F.relu(self.d31(xcat3))\n",
        "        xup42 = F.relu(self.d32(xup41))\n",
        "\n",
        "        xup4 = self.upconv4(xup42)\n",
        "        #xup4 = self.dropout(xup4) # dropout\n",
        "        #xcat4 = torch.cat([xup4, xe12[:,:,2:-3,2:-3]], dim=1)\n",
        "        xcat4 = torch.cat([xup4, xe12], dim=1)\n",
        "        #xcat4=xup4\n",
        "\n",
        "        xup51 = F.relu(self.d41(xcat4))\n",
        "        xup52 = F.relu(self.d42(xup51))\n",
        "\n",
        "        out = self.outconv(xup52)\n",
        "\n",
        "        #output = softmax(out, dim=1)\n",
        "        output=out\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "smbMc8I45k87"
      },
      "outputs": [],
      "source": [
        "#Setting up hyper parameters, from exercise week 6\n",
        "loss_fn =  nn.CrossEntropyLoss()   #Choosing cross entropy loss\n",
        "\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FIFTg1EF_znM"
      },
      "outputs": [],
      "source": [
        "#Training size of 65%\n",
        "#Splitting test and validation set in 50%\n",
        "test_size=0.35\n",
        "batch_size = 16\n",
        "\n",
        "training_idx, test_idx = train_test_split(\n",
        "    range(300),\n",
        "    test_size=test_size,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "test_idx, val_idx = train_test_split(\n",
        "    test_idx,\n",
        "    test_size=0.5,\n",
        "    random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8DPuHYgY_26r"
      },
      "outputs": [],
      "source": [
        "#Gather the training images together\n",
        "\n",
        "training_file_names=[]\n",
        "label_train_names=[]\n",
        "for i in range(len(training_idx)):\n",
        "  if len(str(training_idx[i]))<2:\n",
        "    training_file_names.append('SOCprist000'+str(training_idx[i])+'.tiff')\n",
        "    label_train_names.append('slice__00'+str(training_idx[i])+'.tif')\n",
        "  if len(str(training_idx[i]))==2:\n",
        "    training_file_names.append('SOCprist00'+str(training_idx[i])+'.tiff')\n",
        "    label_train_names.append('slice__0'+str(training_idx[i])+'.tif')\n",
        "  else:\n",
        "    training_file_names.append('SOCprist0'+str(training_idx[i])+'.tiff')\n",
        "    label_train_names.append('slice__'+str(training_idx[i])+'.tif')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "g-Btdx0j_7gA"
      },
      "outputs": [],
      "source": [
        "#Gather the test images together\n",
        "\n",
        "test_file_names=[]\n",
        "label_test_names=[]\n",
        "for i in range(len(test_idx)):\n",
        "  if len(str(test_idx[i]))<2:\n",
        "    test_file_names.append('SOCprist000'+str(test_idx[i])+'.tiff')\n",
        "    label_test_names.append('slice__00'+str(test_idx[i])+'.tif')\n",
        "  if len(str(test_idx[i]))==2:\n",
        "    test_file_names.append('SOCprist00'+str(test_idx[i])+'.tiff')\n",
        "    label_test_names.append('slice__0'+str(test_idx[i])+'.tif')\n",
        "  else:\n",
        "    test_file_names.append('SOCprist0'+str(test_idx[i])+'.tiff')\n",
        "    label_test_names.append('slice__'+str(test_idx[i])+'.tif')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TjXuoXwLPog-"
      },
      "outputs": [],
      "source": [
        "#Gather the validation images together\n",
        "\n",
        "val_file_names=[]\n",
        "label_val_names=[]\n",
        "for i in range(len(val_idx)):\n",
        "  if len(str(val_idx[i]))<2:\n",
        "    val_file_names.append('SOCprist000'+str(val_idx[i])+'.tiff')\n",
        "    label_val_names.append('slice__00'+str(val_idx[i])+'.tif')\n",
        "  if len(str(val_idx[i]))==2:\n",
        "    val_file_names.append('SOCprist00'+str(val_idx[i])+'.tiff')\n",
        "    label_val_names.append('slice__0'+str(val_idx[i])+'.tif')\n",
        "  else:\n",
        "    val_file_names.append('SOCprist0'+str(val_idx[i])+'.tiff')\n",
        "    label_val_names.append('slice__'+str(val_idx[i])+'.tif')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sqCSVV6cE9Un"
      },
      "outputs": [],
      "source": [
        "#Creating the dataset\n",
        "class Trainingdataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None,file_names=None,label_names=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_folder = os.path.join(root_dir, 'data/')\n",
        "        self.label_folder = os.path.join(root_dir, 'labels/')\n",
        "        self.transform = transform\n",
        "        self.file_names= file_names\n",
        "        self.label_names= label_names\n",
        "\n",
        "        self.image_filenames = sorted([f for f in os.listdir(self.image_folder) if f in self.file_names])\n",
        "        self.label_filenames = sorted([f for f in os.listdir(self.label_folder) if f in self.label_names])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      img_name = os.path.join(self.image_folder, self.image_filenames[idx])\n",
        "\n",
        "      number1=img_name[-8:-5] #make sure the label fits with the image\n",
        "      label_name=os.path.join(self.label_folder,'slice__'+str(number1)+'.tif') #finding the corresponding label\n",
        "\n",
        "      #Loading in image and label\n",
        "      image = cv2.imread(img_name, cv2.IMREAD_GRAYSCALE)\n",
        "      label = cv2.imread(label_name, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "      image=np.array(image)\n",
        "      label=np.array(label)\n",
        "\n",
        "      if self.transform is not None: #Inspired by: https://www.youtube.com/watch?v=rAdLwKJBvPM, visited the November 23rd 2023\n",
        "        augmentations=self.transform(image=image,mask=label) #Adding transformations\n",
        "\n",
        "      #Extracting image and label from augmentations\n",
        "      image=augmentations[\"image\"]\n",
        "      label=augmentations[\"mask\"]\n",
        "\n",
        "      return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oqDLd6aV1MIB"
      },
      "outputs": [],
      "source": [
        "#Defining the transformations\n",
        "\n",
        "#Transformation for the training and validation dataset\n",
        "transform = A.Compose(\n",
        "    [\n",
        "     A.RandomCrop(width=128, height=128), #Random cropping 128x128\n",
        "     A.GaussNoise(p=0.05),  # Add Gaussian noise to 5%\n",
        "     A.Normalize(\n",
        "        mean=[0.5],\n",
        "        std=[0.5],\n",
        "        max_pixel_value=255.0,\n",
        "     ), #Normalizing pixels\n",
        "     ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "#The transformations from test only consist of normalization and to tensor transform\n",
        "transform_test = A.Compose(\n",
        "    [\n",
        "     A.Normalize(\n",
        "        mean=[0.5],\n",
        "        std=[0.5],\n",
        "        max_pixel_value=255.0,\n",
        "     ),\n",
        "     ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UpKsXg_j1Gy1"
      },
      "outputs": [],
      "source": [
        "SOC_dataset_train = Trainingdataset(root_dir='drive/My Drive//AI data/', transform = transform,file_names=training_file_names,label_names=label_train_names)\n",
        "SOC_dataset_test = Trainingdataset(root_dir='drive/My Drive//AI data/', transform = transform_test,file_names=test_file_names,label_names=label_test_names)\n",
        "SOC_dataset_val = Trainingdataset(root_dir='drive/My Drive//AI data/', transform = transform,file_names=val_file_names,label_names=label_val_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCwT-256zuCC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEKJRtDj2Omv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2n4gIa2pNim",
        "outputId": "53efc190-d228-4150-8f49-1ea3f5b816ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/806.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/806.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m798.7/806.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.10.0 torchmetrics-1.2.1\n"
          ]
        }
      ],
      "source": [
        "pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZlW8ER1KGNL8"
      },
      "outputs": [],
      "source": [
        "#Importing accuracy metrics\n",
        "from torchmetrics.classification import JaccardIndex\n",
        "from torchmetrics.functional.classification import dice\n",
        "from torchmetrics.classification import MulticlassAccuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97zj0_jLGX1n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gx_50U0KGbSB"
      },
      "outputs": [],
      "source": [
        "batch_size = 16 #Batch size of 16\n",
        "\n",
        "train_loader = DataLoader(SOC_dataset_train, batch_size=batch_size, shuffle=True,drop_last=False)\n",
        "test_loader = DataLoader(SOC_dataset_test, batch_size=batch_size,drop_last=True)\n",
        "val_loader = DataLoader(SOC_dataset_val, batch_size=batch_size,drop_last=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hBjIRQ-eBTNO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3089b2-f87b-4c6f-c5d2-7f929827d36b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 13      training accuracy with jaccard: 0.7436384558677673\n",
            "             training accuracy with dice: 0.8356164693832397\n",
            "             training accuracy with pixel by pixel: 0.8320053815841675\n",
            "             validation accuracy with jaccard: 0.2981192469596863\n",
            "             validation accuracy with dice: 0.5531883239746094\n",
            "             validation accuracy with pixel by pixel: 0.46617090702056885\n",
            "             loss in validation: 1.0329723358154297\n",
            "Step 26      training accuracy with jaccard: 0.9081974029541016\n",
            "             training accuracy with dice: 0.9513652324676514\n",
            "             training accuracy with pixel by pixel: 0.9542719721794128\n",
            "             validation accuracy with jaccard: 0.2955748637517293\n",
            "             validation accuracy with dice: 0.5299326578776041\n",
            "             validation accuracy with pixel by pixel: 0.4767238299051921\n",
            "             loss in validation: 0.7841288447380066\n",
            "Step 39      training accuracy with jaccard: 0.9522387981414795\n",
            "             training accuracy with dice: 0.9753605723381042\n",
            "             training accuracy with pixel by pixel: 0.9762210845947266\n",
            "             validation accuracy with jaccard: 0.7498844464619955\n",
            "             validation accuracy with dice: 0.8578097025553385\n",
            "             validation accuracy with pixel by pixel: 0.8363556861877441\n",
            "             loss in validation: 0.45588016510009766\n",
            "Step 52      training accuracy with jaccard: 0.9468041062355042\n",
            "             training accuracy with dice: 0.9730236530303955\n",
            "             training accuracy with pixel by pixel: 0.9742370843887329\n",
            "             validation accuracy with jaccard: 0.9193917115529379\n",
            "             validation accuracy with dice: 0.9571367899576823\n",
            "             validation accuracy with pixel by pixel: 0.9517843723297119\n",
            "             loss in validation: 0.12670522928237915\n",
            "Step 65      training accuracy with jaccard: 0.9556900858879089\n",
            "             training accuracy with dice: 0.9772840738296509\n",
            "             training accuracy with pixel by pixel: 0.9782677292823792\n",
            "             validation accuracy with jaccard: 0.9530042012532552\n",
            "             validation accuracy with dice: 0.9752184549967448\n",
            "             validation accuracy with pixel by pixel: 0.9738332430521647\n",
            "             loss in validation: 0.07491584867238998\n",
            "Step 78      training accuracy with jaccard: 0.9676830768585205\n",
            "             training accuracy with dice: 0.9836642742156982\n",
            "             training accuracy with pixel by pixel: 0.9846245050430298\n",
            "             validation accuracy with jaccard: 0.9582539399464926\n",
            "             validation accuracy with dice: 0.9786567687988281\n",
            "             validation accuracy with pixel by pixel: 0.9762296676635742\n",
            "             loss in validation: 0.06021668389439583\n",
            "Step 91      training accuracy with jaccard: 0.9596978425979614\n",
            "             training accuracy with dice: 0.9797122478485107\n",
            "             training accuracy with pixel by pixel: 0.9808530211448669\n",
            "             validation accuracy with jaccard: 0.9686972300211588\n",
            "             validation accuracy with dice: 0.984130859375\n",
            "             validation accuracy with pixel by pixel: 0.9818887710571289\n",
            "             loss in validation: 0.04679597541689873\n",
            "Step 104     training accuracy with jaccard: 0.9619442820549011\n",
            "             training accuracy with dice: 0.9805136322975159\n",
            "             training accuracy with pixel by pixel: 0.9824103713035583\n",
            "             validation accuracy with jaccard: 0.9730656941731771\n",
            "             validation accuracy with dice: 0.9862136840820312\n",
            "             validation accuracy with pixel by pixel: 0.9862424532572428\n",
            "             loss in validation: 0.042954713106155396\n",
            "Step 117     training accuracy with jaccard: 0.961378812789917\n",
            "             training accuracy with dice: 0.9800896048545837\n",
            "             training accuracy with pixel by pixel: 0.9814037084579468\n",
            "             validation accuracy with jaccard: 0.9688706398010254\n",
            "             validation accuracy with dice: 0.9838816324869791\n",
            "             validation accuracy with pixel by pixel: 0.9855759938557943\n",
            "             loss in validation: 0.04593421146273613\n",
            "Step 130     training accuracy with jaccard: 0.9664022922515869\n",
            "             training accuracy with dice: 0.982824444770813\n",
            "             training accuracy with pixel by pixel: 0.9843676090240479\n",
            "             validation accuracy with jaccard: 0.9736812909444174\n",
            "             validation accuracy with dice: 0.9864069620768229\n",
            "             validation accuracy with pixel by pixel: 0.9862574736277262\n",
            "             loss in validation: 0.04305353760719299\n",
            "Step 143     training accuracy with jaccard: 0.9699667692184448\n",
            "             training accuracy with dice: 0.9846649169921875\n",
            "             training accuracy with pixel by pixel: 0.9851996898651123\n",
            "             validation accuracy with jaccard: 0.9781954288482666\n",
            "             validation accuracy with dice: 0.9888509114583334\n",
            "             validation accuracy with pixel by pixel: 0.9887929757436117\n",
            "             loss in validation: 0.03337452560663223\n",
            "Step 156     training accuracy with jaccard: 0.96980881690979\n",
            "             training accuracy with dice: 0.984674870967865\n",
            "             training accuracy with pixel by pixel: 0.9855244755744934\n",
            "             validation accuracy with jaccard: 0.9787077903747559\n",
            "             validation accuracy with dice: 0.9891713460286459\n",
            "             validation accuracy with pixel by pixel: 0.9893984794616699\n",
            "             loss in validation: 0.03173916041851044\n",
            "Step 169     training accuracy with jaccard: 0.9677101969718933\n",
            "             training accuracy with dice: 0.9834533333778381\n",
            "             training accuracy with pixel by pixel: 0.9846590757369995\n",
            "             validation accuracy with jaccard: 0.9762751261393229\n",
            "             validation accuracy with dice: 0.9879124959309896\n",
            "             validation accuracy with pixel by pixel: 0.9891269207000732\n",
            "             loss in validation: 0.03292326256632805\n",
            "Step 182     training accuracy with jaccard: 0.9663298726081848\n",
            "             training accuracy with dice: 0.9828112721443176\n",
            "             training accuracy with pixel by pixel: 0.9835883378982544\n",
            "             validation accuracy with jaccard: 0.9747888247172037\n",
            "             validation accuracy with dice: 0.987341562906901\n",
            "             validation accuracy with pixel by pixel: 0.9869041442871094\n",
            "             loss in validation: 0.03421453759074211\n",
            "Step 195     training accuracy with jaccard: 0.9625177383422852\n",
            "             training accuracy with dice: 0.9807337522506714\n",
            "             training accuracy with pixel by pixel: 0.9833220839500427\n",
            "             validation accuracy with jaccard: 0.9734136263529459\n",
            "             validation accuracy with dice: 0.9864120483398438\n",
            "             validation accuracy with pixel by pixel: 0.9856732686360677\n",
            "             loss in validation: 0.04064294323325157\n",
            "Step 208     training accuracy with jaccard: 0.9729480147361755\n",
            "             training accuracy with dice: 0.9863366484642029\n",
            "             training accuracy with pixel by pixel: 0.9868726134300232\n",
            "             validation accuracy with jaccard: 0.9767531553904215\n",
            "             validation accuracy with dice: 0.9880561828613281\n",
            "             validation accuracy with pixel by pixel: 0.988064686457316\n",
            "             loss in validation: 0.03397657349705696\n",
            "Step 221     training accuracy with jaccard: 0.9742410182952881\n",
            "             training accuracy with dice: 0.9868307709693909\n",
            "             training accuracy with pixel by pixel: 0.9874020218849182\n",
            "             validation accuracy with jaccard: 0.9748689333597819\n",
            "             validation accuracy with dice: 0.9872296651204427\n",
            "             validation accuracy with pixel by pixel: 0.9873329003651937\n",
            "             loss in validation: 0.03276645019650459\n",
            "Step 234     training accuracy with jaccard: 0.973457932472229\n",
            "             training accuracy with dice: 0.9865913391113281\n",
            "             training accuracy with pixel by pixel: 0.987424910068512\n",
            "             validation accuracy with jaccard: 0.975764274597168\n",
            "             validation accuracy with dice: 0.9876874287923177\n",
            "             validation accuracy with pixel by pixel: 0.9871546427408854\n",
            "             loss in validation: 0.033769771456718445\n",
            "Step 247     training accuracy with jaccard: 0.9709652662277222\n",
            "             training accuracy with dice: 0.9853855967521667\n",
            "             training accuracy with pixel by pixel: 0.9863187074661255\n",
            "             validation accuracy with jaccard: 0.9759510358174642\n",
            "             validation accuracy with dice: 0.9877751668294271\n",
            "             validation accuracy with pixel by pixel: 0.9874014059702555\n",
            "             loss in validation: 0.03414814546704292\n",
            "Step 260     training accuracy with jaccard: 0.9734276533126831\n",
            "             training accuracy with dice: 0.9865980744361877\n",
            "             training accuracy with pixel by pixel: 0.9869769215583801\n",
            "             validation accuracy with jaccard: 0.9749427636464437\n",
            "             validation accuracy with dice: 0.987433115641276\n",
            "             validation accuracy with pixel by pixel: 0.9869960149129232\n",
            "             loss in validation: 0.03373539447784424\n",
            "Step 273     training accuracy with jaccard: 0.9747222661972046\n",
            "             training accuracy with dice: 0.9871042966842651\n",
            "             training accuracy with pixel by pixel: 0.9877309203147888\n",
            "             validation accuracy with jaccard: 0.9777399698893229\n",
            "             validation accuracy with dice: 0.9887962341308594\n",
            "             validation accuracy with pixel by pixel: 0.988439162572225\n",
            "             loss in validation: 0.030133409425616264\n",
            "Step 286     training accuracy with jaccard: 0.9739711284637451\n",
            "             training accuracy with dice: 0.986780047416687\n",
            "             training accuracy with pixel by pixel: 0.9873741865158081\n",
            "             validation accuracy with jaccard: 0.9732449849446615\n",
            "             validation accuracy with dice: 0.9863204956054688\n",
            "             validation accuracy with pixel by pixel: 0.9862030347188314\n",
            "             loss in validation: 0.03655977547168732\n",
            "Step 299     training accuracy with jaccard: 0.973979115486145\n",
            "             training accuracy with dice: 0.986763596534729\n",
            "             training accuracy with pixel by pixel: 0.9874508380889893\n",
            "             validation accuracy with jaccard: 0.9781901041666666\n",
            "             validation accuracy with dice: 0.9890467325846354\n",
            "             validation accuracy with pixel by pixel: 0.9889825979868571\n",
            "             loss in validation: 0.029897525906562805\n",
            "Step 312     training accuracy with jaccard: 0.9741143584251404\n",
            "             training accuracy with dice: 0.9867691397666931\n",
            "             training accuracy with pixel by pixel: 0.9875949025154114\n",
            "             validation accuracy with jaccard: 0.974586009979248\n",
            "             validation accuracy with dice: 0.9870071411132812\n",
            "             validation accuracy with pixel by pixel: 0.9870080153147379\n",
            "             loss in validation: 0.03636111319065094\n",
            "Step 325     training accuracy with jaccard: 0.9734070897102356\n",
            "             training accuracy with dice: 0.986464262008667\n",
            "             training accuracy with pixel by pixel: 0.9870613813400269\n",
            "             validation accuracy with jaccard: 0.9768610795338949\n",
            "             validation accuracy with dice: 0.9881490071614584\n",
            "             validation accuracy with pixel by pixel: 0.9885783195495605\n",
            "             loss in validation: 0.03162088990211487\n",
            "Step 338     training accuracy with jaccard: 0.9724708199501038\n",
            "             training accuracy with dice: 0.985731840133667\n",
            "             training accuracy with pixel by pixel: 0.9870842695236206\n",
            "             validation accuracy with jaccard: 0.9769090811411539\n",
            "             validation accuracy with dice: 0.9882888793945312\n",
            "             validation accuracy with pixel by pixel: 0.98775847752889\n",
            "             loss in validation: 0.03309370204806328\n",
            "Step 351     training accuracy with jaccard: 0.9682783484458923\n",
            "             training accuracy with dice: 0.9837270975112915\n",
            "             training accuracy with pixel by pixel: 0.9855719804763794\n",
            "             validation accuracy with jaccard: 0.9759728908538818\n",
            "             validation accuracy with dice: 0.9877548217773438\n",
            "             validation accuracy with pixel by pixel: 0.9873183568318685\n",
            "             loss in validation: 0.03222707286477089\n",
            "Step 364     training accuracy with jaccard: 0.9746583700180054\n",
            "             training accuracy with dice: 0.987229585647583\n",
            "             training accuracy with pixel by pixel: 0.9880891442298889\n",
            "             validation accuracy with jaccard: 0.9744329452514648\n",
            "             validation accuracy with dice: 0.98699951171875\n",
            "             validation accuracy with pixel by pixel: 0.9866377512613932\n",
            "             loss in validation: 0.03443038836121559\n",
            "Step 377     training accuracy with jaccard: 0.9762244820594788\n",
            "             training accuracy with dice: 0.9878572225570679\n",
            "             training accuracy with pixel by pixel: 0.9879716634750366\n",
            "             validation accuracy with jaccard: 0.9751553535461426\n",
            "             validation accuracy with dice: 0.987335205078125\n",
            "             validation accuracy with pixel by pixel: 0.9875532786051432\n",
            "             loss in validation: 0.03221151605248451\n",
            "Step 390     training accuracy with jaccard: 0.9744392037391663\n",
            "             training accuracy with dice: 0.986968994140625\n",
            "             training accuracy with pixel by pixel: 0.9875943660736084\n",
            "             validation accuracy with jaccard: 0.9743730227152506\n",
            "             validation accuracy with dice: 0.9868443806966146\n",
            "             validation accuracy with pixel by pixel: 0.9870152473449707\n",
            "             loss in validation: 0.03402598202228546\n",
            "Step 403     training accuracy with jaccard: 0.975408136844635\n",
            "             training accuracy with dice: 0.9878390431404114\n",
            "             training accuracy with pixel by pixel: 0.9888627529144287\n",
            "             validation accuracy with jaccard: 0.9783972104390463\n",
            "             validation accuracy with dice: 0.9889691670735677\n",
            "             validation accuracy with pixel by pixel: 0.9888946215311686\n",
            "             loss in validation: 0.029197821393609047\n",
            "Step 416     training accuracy with jaccard: 0.9762220978736877\n",
            "             training accuracy with dice: 0.987978994846344\n",
            "             training accuracy with pixel by pixel: 0.9883372783660889\n",
            "             validation accuracy with jaccard: 0.9764814376831055\n",
            "             validation accuracy with dice: 0.9880727132161459\n",
            "             validation accuracy with pixel by pixel: 0.9875828425089518\n",
            "             loss in validation: 0.030489662662148476\n",
            "Step 429     training accuracy with jaccard: 0.9706225991249084\n",
            "             training accuracy with dice: 0.9850968718528748\n",
            "             training accuracy with pixel by pixel: 0.9861453175544739\n",
            "             validation accuracy with jaccard: 0.9769805272420248\n",
            "             validation accuracy with dice: 0.9882850646972656\n",
            "             validation accuracy with pixel by pixel: 0.9878542423248291\n",
            "             loss in validation: 0.03089175559580326\n",
            "Step 442     training accuracy with jaccard: 0.969892144203186\n",
            "             training accuracy with dice: 0.984963059425354\n",
            "             training accuracy with pixel by pixel: 0.9860020875930786\n",
            "             validation accuracy with jaccard: 0.974067211151123\n",
            "             validation accuracy with dice: 0.986883799235026\n",
            "             validation accuracy with pixel by pixel: 0.9879180590311686\n",
            "             loss in validation: 0.033020827919244766\n",
            "Step 455     training accuracy with jaccard: 0.9728965759277344\n",
            "             training accuracy with dice: 0.9862673878669739\n",
            "             training accuracy with pixel by pixel: 0.9868385791778564\n",
            "             validation accuracy with jaccard: 0.9779088497161865\n",
            "             validation accuracy with dice: 0.9887262980143229\n",
            "             validation accuracy with pixel by pixel: 0.9890041351318359\n",
            "             loss in validation: 0.029147105291485786\n",
            "Step 468     training accuracy with jaccard: 0.9756079316139221\n",
            "             training accuracy with dice: 0.9876095652580261\n",
            "             training accuracy with pixel by pixel: 0.9883829355239868\n",
            "             validation accuracy with jaccard: 0.9781084060668945\n",
            "             validation accuracy with dice: 0.9888814290364584\n",
            "             validation accuracy with pixel by pixel: 0.9882580439249674\n",
            "             loss in validation: 0.029346024617552757\n",
            "Step 481     training accuracy with jaccard: 0.9774914979934692\n",
            "             training accuracy with dice: 0.9885429739952087\n",
            "             training accuracy with pixel by pixel: 0.9890559911727905\n",
            "             validation accuracy with jaccard: 0.9776654243469238\n",
            "             validation accuracy with dice: 0.9888648986816406\n",
            "             validation accuracy with pixel by pixel: 0.9886389573415121\n",
            "             loss in validation: 0.028854167088866234\n",
            "Step 494     training accuracy with jaccard: 0.9764607548713684\n",
            "             training accuracy with dice: 0.988064706325531\n",
            "             training accuracy with pixel by pixel: 0.9884793758392334\n",
            "             validation accuracy with jaccard: 0.9782929420471191\n",
            "             validation accuracy with dice: 0.9889233907063802\n",
            "             validation accuracy with pixel by pixel: 0.9885093371073405\n",
            "             loss in validation: 0.03065677173435688\n",
            "Step 507     training accuracy with jaccard: 0.9775766134262085\n",
            "             training accuracy with dice: 0.9886512756347656\n",
            "             training accuracy with pixel by pixel: 0.9893075227737427\n",
            "             validation accuracy with jaccard: 0.9777949651082357\n",
            "             validation accuracy with dice: 0.9887135823567709\n",
            "             validation accuracy with pixel by pixel: 0.988353411356608\n",
            "             loss in validation: 0.029385631904006004\n",
            "Step 520     training accuracy with jaccard: 0.9734249711036682\n",
            "             training accuracy with dice: 0.9866870045661926\n",
            "             training accuracy with pixel by pixel: 0.987238347530365\n",
            "             validation accuracy with jaccard: 0.9763339360555013\n",
            "             validation accuracy with dice: 0.9879849751790365\n",
            "             validation accuracy with pixel by pixel: 0.9877559343973795\n",
            "             loss in validation: 0.030070900917053223\n",
            "Step 533     training accuracy with jaccard: 0.9777297973632812\n",
            "             training accuracy with dice: 0.988692045211792\n",
            "             training accuracy with pixel by pixel: 0.9887625575065613\n",
            "             validation accuracy with jaccard: 0.976628303527832\n",
            "             validation accuracy with dice: 0.9882189432779948\n",
            "             validation accuracy with pixel by pixel: 0.9879791736602783\n",
            "             loss in validation: 0.030302951112389565\n",
            "Step 546     training accuracy with jaccard: 0.9768944978713989\n",
            "             training accuracy with dice: 0.9882469177246094\n",
            "             training accuracy with pixel by pixel: 0.9886208772659302\n",
            "             validation accuracy with jaccard: 0.9776047865549723\n",
            "             validation accuracy with dice: 0.9888089497884115\n",
            "             validation accuracy with pixel by pixel: 0.9885352452596029\n",
            "             loss in validation: 0.02889430709183216\n",
            "Step 559     training accuracy with jaccard: 0.9744717478752136\n",
            "             training accuracy with dice: 0.987024188041687\n",
            "             training accuracy with pixel by pixel: 0.9876621961593628\n",
            "             validation accuracy with jaccard: 0.9804370403289795\n",
            "             validation accuracy with dice: 0.9899991353352865\n",
            "             validation accuracy with pixel by pixel: 0.9900832970937093\n",
            "             loss in validation: 0.025354275479912758\n",
            "Step 572     training accuracy with jaccard: 0.9755867719650269\n",
            "             training accuracy with dice: 0.987554132938385\n",
            "             training accuracy with pixel by pixel: 0.9883399605751038\n",
            "             validation accuracy with jaccard: 0.9751976331075033\n",
            "             validation accuracy with dice: 0.987274169921875\n",
            "             validation accuracy with pixel by pixel: 0.9876507918039957\n",
            "             loss in validation: 0.03195217624306679\n",
            "Step 585     training accuracy with jaccard: 0.9773823022842407\n",
            "             training accuracy with dice: 0.9885145425796509\n",
            "             training accuracy with pixel by pixel: 0.9888862371444702\n",
            "             validation accuracy with jaccard: 0.9762231508890787\n",
            "             validation accuracy with dice: 0.9879557291666666\n",
            "             validation accuracy with pixel by pixel: 0.9878360430399576\n",
            "             loss in validation: 0.03162030875682831\n",
            "Step 598     training accuracy with jaccard: 0.9775689840316772\n",
            "             training accuracy with dice: 0.9886524677276611\n",
            "             training accuracy with pixel by pixel: 0.9889404773712158\n",
            "             validation accuracy with jaccard: 0.9787424405415853\n",
            "             validation accuracy with dice: 0.9890874226888021\n",
            "             validation accuracy with pixel by pixel: 0.9891277949015299\n",
            "             loss in validation: 0.028449291363358498\n",
            "Step 611     training accuracy with jaccard: 0.9763287901878357\n",
            "             training accuracy with dice: 0.987909734249115\n",
            "             training accuracy with pixel by pixel: 0.9883700609207153\n",
            "             validation accuracy with jaccard: 0.979417641957601\n",
            "             validation accuracy with dice: 0.9894790649414062\n",
            "             validation accuracy with pixel by pixel: 0.9897233645121256\n",
            "             loss in validation: 0.02721407450735569\n",
            "Step 624     training accuracy with jaccard: 0.9769101738929749\n",
            "             training accuracy with dice: 0.9882786273956299\n",
            "             training accuracy with pixel by pixel: 0.988799512386322\n",
            "             validation accuracy with jaccard: 0.978412946065267\n",
            "             validation accuracy with dice: 0.9889361063639323\n",
            "             validation accuracy with pixel by pixel: 0.989342212677002\n",
            "             loss in validation: 0.029141245409846306\n",
            "Step 637     training accuracy with jaccard: 0.9770567417144775\n",
            "             training accuracy with dice: 0.9883778095245361\n",
            "             training accuracy with pixel by pixel: 0.9886812567710876\n",
            "             validation accuracy with jaccard: 0.9794526100158691\n",
            "             validation accuracy with dice: 0.9896837870279948\n",
            "             validation accuracy with pixel by pixel: 0.989311695098877\n",
            "             loss in validation: 0.025888079777359962\n",
            "Step 650     training accuracy with jaccard: 0.977635383605957\n",
            "             training accuracy with dice: 0.9886304140090942\n",
            "             training accuracy with pixel by pixel: 0.9891493320465088\n",
            "             validation accuracy with jaccard: 0.9787851174672445\n",
            "             validation accuracy with dice: 0.989190419514974\n",
            "             validation accuracy with pixel by pixel: 0.9890117645263672\n",
            "             loss in validation: 0.027645403519272804\n",
            "Finished training.\n"
          ]
        }
      ],
      "source": [
        "#Parts of this code section is from exercise 4.2-EXE-CNN-CIFAR-10.ipynb\n",
        "net=UNet(n_class=3)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "net.to(device)\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.0001) #choosing the Adam optimizzer\n",
        "\n",
        "#Defining the different accuracy metrics\n",
        "jaccard=JaccardIndex(task=\"multiclass\", num_classes=3).to(device) #jaccard\n",
        "accuracy=MulticlassAccuracy(num_classes=3).to(device) #pixel wise\n",
        "num_epochs = 50\n",
        "validation_every_steps = np.ceil(len(train_loader.dataset)/batch_size) #How often the validation should be\n",
        "\n",
        "step = 0\n",
        "net.train()\n",
        "\n",
        "#Allocating list for accuracy measures\n",
        "train_accuracies_jaccard = []\n",
        "train_accuracies_dice = []\n",
        "train_accuracies_pixel = []\n",
        "valid_accuracies_jaccard = []\n",
        "valid_accuracies_dice = []\n",
        "valid_accuracies_pixel = []\n",
        "loss_epochs=[]\n",
        "val_losses=[]\n",
        "loss_train=[]\n",
        "val_loss=[]\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_accuracies_batches_jaccard = []\n",
        "    train_accuracies_batches_dice = []\n",
        "    train_accuracies_batches_pixel = []\n",
        "    loss_epochs=[]\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        #Forward pass\n",
        "        output = net(inputs)\n",
        "\n",
        "        #Divide targets into classes: [0,1,2]\n",
        "        un_target=targets.unique()\n",
        "        targets[targets==un_target[0]]=0\n",
        "        targets[targets==un_target[1]]=1\n",
        "        targets[targets==un_target[2]]=2\n",
        "\n",
        "        targets = targets.to(torch.int64)\n",
        "\n",
        "        #compute loss function\n",
        "        loss = loss_fn(output, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        step += 1\n",
        "\n",
        "        # Compute accuracy. Note the adding of the softmax function to obtain the probabilities\n",
        "        predictions = torch.argmax(softmax(output,dim=1),dim=1) #Prediction with max of softmax output\n",
        "        train_accuracies_batches_dice.append(dice(predictions,targets).cpu())\n",
        "        train_accuracies_batches_jaccard.append(jaccard(predictions,targets).cpu())\n",
        "        train_accuracies_batches_pixel.append(accuracy(predictions,targets).cpu())\n",
        "        loss_epochs.append(loss.detach().cpu().numpy())\n",
        "\n",
        "        if step % validation_every_steps == 0:\n",
        "\n",
        "            # Append average training accuracy to list.\n",
        "            train_accuracies_jaccard.append(np.mean(train_accuracies_batches_jaccard))\n",
        "            train_accuracies_dice.append(np.mean(train_accuracies_batches_dice))\n",
        "            train_accuracies_pixel.append(np.mean(train_accuracies_batches_pixel))\n",
        "            loss_train.append(np.mean(loss_epochs))\n",
        "\n",
        "            train_accuracies_batches_jaccard = []\n",
        "            train_accuracies_batches_dice = []\n",
        "            train_accuracies_batches_pixel = []\n",
        "\n",
        "            # Compute accuracies on validation set.\n",
        "            valid_accuracies_batches_jaccard = []\n",
        "            valid_accuracies_batches_dice = []\n",
        "            valid_accuracies_batches_pixel = []\n",
        "            val_losses=[]\n",
        "            with torch.no_grad():\n",
        "                net.eval()\n",
        "                for inputs, targets in val_loader:\n",
        "                    inputs, targets = inputs.to(device), targets.to(device)\n",
        "                    output = net(inputs)\n",
        "\n",
        "                    un_target=targets.unique()\n",
        "                    # Compute loss.\n",
        "                    targets[targets==un_target[0]]=0\n",
        "                    targets[targets==un_target[1]]=1\n",
        "                    targets[targets==un_target[2]]=2\n",
        "\n",
        "                    targets = targets.to(torch.int64)\n",
        "\n",
        "                    loss = loss_fn(output, targets)\n",
        "\n",
        "                    predictions = torch.argmax(softmax(output,dim=1),dim=1)\n",
        "                    valid_accuracies_batches_dice.append(dice(predictions,targets).cpu())\n",
        "                    valid_accuracies_batches_jaccard.append(jaccard(predictions,targets).cpu())\n",
        "                    valid_accuracies_batches_pixel.append(accuracy(predictions,targets).cpu())\n",
        "                    val_losses.append(loss.cpu())\n",
        "                net.train()\n",
        "\n",
        "            # Append average validation accuracy to list.\n",
        "            valid_accuracies_jaccard.append(np.sum(valid_accuracies_batches_jaccard) / len(val_loader))\n",
        "            valid_accuracies_dice.append(np.sum(valid_accuracies_batches_dice) / len(val_loader))\n",
        "            valid_accuracies_pixel.append(np.sum(valid_accuracies_batches_pixel) / len(val_loader))\n",
        "            val_loss.append(np.mean(val_losses))\n",
        "\n",
        "            print(f\"Step {step:<5}   training accuracy with jaccard: {train_accuracies_jaccard[-1]}\")\n",
        "            print(f\"             training accuracy with dice: {train_accuracies_dice[-1]}\")\n",
        "            print(f\"             training accuracy with pixel by pixel: {train_accuracies_pixel[-1]}\")\n",
        "            print(f\"             validation accuracy with jaccard: {valid_accuracies_jaccard[-1]}\")\n",
        "            print(f\"             validation accuracy with dice: {valid_accuracies_dice[-1]}\")\n",
        "            print(f\"             validation accuracy with pixel by pixel: {valid_accuracies_pixel[-1]}\")\n",
        "            print(f\"             loss in validation: {val_loss[-1]}\")\n",
        "\n",
        "\n",
        "print(\"Finished training.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving the model\n",
        "\n",
        "#torch.save(net,'./2dunet.pth')\n",
        "\n",
        "#model = torch.load('./2dunet.pth')\n"
      ],
      "metadata": {
        "id": "Nd89A1Aa3Ogz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-9IFbgvkk6-"
      },
      "outputs": [],
      "source": [
        "test_dice=0\n",
        "test_jaccard=0\n",
        "test_pixel=0\n",
        "\n",
        "#Finding the test accuracy for all test images\n",
        "for inputs, targets in test_loader:\n",
        "  inputs_val, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "  un_target=targets.unique()\n",
        "  # Compute loss.\n",
        "  targets[targets==un_target[0]]=0\n",
        "  targets[targets==un_target[1]]=1\n",
        "  targets[targets==un_target[2]]=2\n",
        "\n",
        "  targets_val = targets.to(torch.int64)\n",
        "\n",
        "  #Determine kernel size and stride to reconstruct original image size\n",
        "  kernel_size = 128\n",
        "  stride = 128\n",
        "\n",
        "\n",
        "  #Calculating need for padding\n",
        "  pad_min=int(np.floor((kernel_size*np.ceil(inputs_val.size(-1)/kernel_size)-inputs_val.size(-1))/2))\n",
        "  pad_max=int(np.ceil((kernel_size*np.ceil(inputs_val.size(-1)/kernel_size)-inputs_val.size(-1))/2))\n",
        "\n",
        "  inputs_val=torch.nn.functional.pad(inputs_val,pad=(pad_min,pad_max,pad_min,pad_max)) #to get the size to 512 (4*128)\n",
        "  targets_val=torch.nn.functional.pad(targets_val,pad=(pad_min,pad_max,pad_min,pad_max)) #to get the size to 512 (4*128)\n",
        "\n",
        "  #Source: https://discuss.pytorch.org/t/how-to-split-tensors-with-overlap-and-then-reconstruct-the-original-tensor/70261/7?fbclid=IwAR1rdUAuDnUpVm2OwmXRaFo-l2AMLJ1RLn5bJEp6f1JcU7wR5CHpugMHc6Y\n",
        "  #Visited the 23rd of November 2023\n",
        "  B, C, W, H = inputs_val.size(0), inputs_val.size(1), inputs_val.size(2), inputs_val.size(3)\n",
        "\n",
        "  #Source: https://discuss.pytorch.org/t/how-to-split-tensors-with-overlap-and-then-reconstruct-the-original-tensor/70261/7?fbclid=IwAR1rdUAuDnUpVm2OwmXRaFo-l2AMLJ1RLn5bJEp6f1JcU7wR5CHpugMHc6Y\n",
        "  #Visited the 23rd of November 2023\n",
        "\n",
        "  #Splitting target and test images into kernels\n",
        "  images_split_1 = inputs_val.unfold(3, kernel_size, stride).unfold(2, kernel_size, stride).permute(0,1,2,3,5,4)\n",
        "  targets_split=targets_val.unfold(2, kernel_size, stride).unfold(1, kernel_size, stride).permute(0,1,2,4,3)\n",
        "\n",
        "  preds=torch.empty(images_split_1.size(0),images_split_1.size(2),images_split_1.size(3),images_split_1.size(-1), images_split_1.size(-1))\n",
        "  tar=targets_split\n",
        "\n",
        "  for i in range(images_split_1.size(2)):\n",
        "    for j in range(images_split_1.size(3)):\n",
        "      output_val = model(images_split_1[:,:,i,j,:,:])\n",
        "      predicted_val = softmax(output_val,dim=1).max(1)[1]\n",
        "\n",
        "      preds[:,i,j,:,:]=predicted_val\n",
        "\n",
        "  #Source: https://discuss.pytorch.org/t/how-to-split-tensors-with-overlap-and-then-reconstruct-the-original-tensor/70261/7?fbclid=IwAR1rdUAuDnUpVm2OwmXRaFo-l2AMLJ1RLn5bJEp6f1JcU7wR5CHpugMHc6Y\n",
        "  #Visited the 23rd of November 2023\n",
        "\n",
        "  #Patching images back together\n",
        "  patches = preds.contiguous().view(B, -1, kernel_size*kernel_size)\n",
        "  patches = patches.permute(0,2,1)\n",
        "  patches = patches.contiguous().view(B, C*kernel_size*kernel_size, -1)\n",
        "\n",
        "\n",
        "  output = torch.nn.functional.fold(\n",
        "      patches, output_size=(H, W), kernel_size=kernel_size, stride=stride)\n",
        "  output = output.to(torch.int64)\n",
        "  for i in range(output.size(0)):\n",
        "    test_dice+=dice(output[i,0,pad_min:-pad_max,pad_min:-pad_max],targets_val[i,pad_min:-pad_max,pad_min:-pad_max].cpu())\n",
        "    test_jaccard+=jaccard(output[i,0,pad_min:-pad_max,pad_min:-pad_max].to(device),targets_val[i,pad_min:-pad_max,pad_min:-pad_max])\n",
        "    test_pixel+=accuracy(output[i,0,pad_min:-pad_max,pad_min:-pad_max].to(device),targets_val[i,pad_min:-pad_max,pad_min:-pad_max])\n",
        "\n",
        "#Final test accuracies\n",
        "tdice=test_dice/(batch_size*len(val_loader))\n",
        "tjaccard=test_jaccard/(batch_size*len(val_loader))\n",
        "tpixel=test_pixel/(batch_size*len(val_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ngg4MboP4KYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uGNOJLejdVm"
      },
      "outputs": [],
      "source": [
        "preds=torch.empty(images_split_1.size(0),images_split_1.size(2),images_split_1.size(3),images_split_1.size(-1), images_split_1.size(-1))\n",
        "tar=targets_split\n",
        "\n",
        "import matplotlib.pylab as pylab\n",
        "params = {'legend.fontsize': 'x-large',\n",
        "          'figure.figsize': (15, 5),\n",
        "         'axes.labelsize': 'x-large',\n",
        "         'axes.titlesize':'x-large',\n",
        "         'xtick.labelsize':'x-large',\n",
        "         'ytick.labelsize':'x-large'}\n",
        "pylab.rcParams.update(params)\n",
        "\n",
        "#Plotting the label probabilities, predicted labels and difference between labels for 128x128 kernels of the test images\n",
        "for i in range(images_split_1.size(2)):\n",
        "  for j in range(images_split_1.size(3)):\n",
        "    output_val = net(images_split_1[:,:,i,j,:,:])\n",
        "    predicted_val = softmax(output_val,dim=1).max(1)[1]\n",
        "\n",
        "    output=softmax(output_val,dim=1)\n",
        "    targets=tar[0,i,j,:,:]\n",
        "    predicted=predicted_val\n",
        "    inputs=images_split_1[:,:,i,j,:,:]\n",
        "\n",
        "    fig,axs = plt.subplots(2,2,figsize=(15,10),dpi=200)\n",
        "    im1=axs[0,0].imshow(output.detach().cpu().numpy()[0,0,:,:])\n",
        "    axs[0,0].set_title('Label 1, probability')\n",
        "    axs[0,0].set_xlabel('Number of pixels [#]')\n",
        "    axs[0,0].set_ylabel('Number of pixels [#]')\n",
        "    cbar1=plt.colorbar(im1, ax=axs[0,0],label='Probability', pad=0.1)\n",
        "    im1.set_clim(0,1)\n",
        "    im2=axs[1,0].imshow(output.detach().cpu().numpy()[0,1,:,:])\n",
        "    axs[1,0].set_title('Label 2, probability')\n",
        "    plt.colorbar(im2, ax=axs[1,0],label='Probability', pad=0.1)\n",
        "    im2.set_clim(0,1)\n",
        "    im3=axs[0,1].imshow(output.detach().cpu().numpy()[0,2,:,:])\n",
        "    axs[1,0].set_xlabel('Number of pixels [#]')\n",
        "    axs[1,0].set_ylabel('Number of pixels [#]')\n",
        "    axs[0,1].set_title('Label 3,probabiliy')\n",
        "    plt.colorbar(im3, ax=axs[0,1],label='Probability', pad=0.1)\n",
        "    im3.set_clim(0,1)\n",
        "    axs[0,1].set_xlabel('Number of pixels [#]')\n",
        "    axs[0,1].set_ylabel('Number of pixels [#]')\n",
        "    im4=axs[1,1].imshow(targets.cpu())\n",
        "    axs[1,1].set_title('Target labels')\n",
        "    axs[1,1].set_xlabel('Number of pixels [#]')\n",
        "    axs[1,1].set_ylabel('Number of pixels [#]')\n",
        "    plt.colorbar(im4, ax=axs[1,1],ticks=[0,1,2],label='Label number', pad=0.1)\n",
        "    #fig.suptitle('Probability distribution for each class', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    fig,axs=plt.subplots(2,2,figsize=(15,10),dpi=200)\n",
        "    im1=axs[1,0].imshow(predicted[0,:,:].cpu())\n",
        "    axs[1,0].set_title('Predicted labels')\n",
        "    axs[1,0].set_xlabel('Number of pixels [#]')\n",
        "    axs[1,0].set_ylabel('Number of pixels [#]')\n",
        "    plt.colorbar(im1, ax=axs[1, 0],ticks=[0,1,2],label='Label number', pad=0.1)\n",
        "    im2=axs[1,1].imshow(targets.cpu())\n",
        "    axs[1,1].set_title('Target labels')\n",
        "    plt.colorbar(im2, ax=axs[1,1],ticks=[0,1,2],label='Label number', pad=0.1)\n",
        "    axs[1,1].set_xlabel('Number of pixels [#]')\n",
        "    axs[1,1].set_ylabel('Number of pixels [#]')\n",
        "    im3=axs[0,0].imshow(inputs[0,0,:,:].cpu())\n",
        "    axs[0,0].set_title('Original image')\n",
        "    plt.colorbar(im3, ax=axs[0,0],label='Normalized intensities', pad=0.1)\n",
        "    axs[0,0].set_xlabel('Number of pixels [#]')\n",
        "    axs[0,0].set_ylabel('Number of pixels [#]')\n",
        "    im4=axs[0,1].imshow(np.abs(targets.detach().cpu().numpy()-predicted[0,:,:].detach().cpu().numpy()))\n",
        "    axs[0,1].set_title('Difference in labels')\n",
        "    axs[0,1].set_xlabel('Number of pixels [#]')\n",
        "    axs[0,1].set_ylabel('Number of pixels [#]')\n",
        "    plt.colorbar(im4, ax=axs[0,1],ticks=[0,1,2],label='Absolute difference in labels', pad=0.1)\n",
        "\n",
        "    #fig.suptitle('Comparison of target and predicted mask', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    preds[:,i,j,:,:]=predicted_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVYJGvau6z6G"
      },
      "outputs": [],
      "source": [
        "#Source: https://discuss.pytorch.org/t/how-to-split-tensors-with-overlap-and-then-reconstruct-the-original-tensor/70261/7?fbclid=IwAR1rdUAuDnUpVm2OwmXRaFo-l2AMLJ1RLn5bJEp6f1JcU7wR5CHpugMHc6Y\n",
        "#Visited the 23rd of November 2023\n",
        "patches = preds.contiguous().view(B, -1, kernel_size*kernel_size)\n",
        "patches = patches.permute(0,2,1)\n",
        "patches = patches.contiguous().view(B, C*kernel_size*kernel_size, -1)\n",
        "\n",
        "output = torch.nn.functional.fold(\n",
        "    patches, output_size=(H, W), kernel_size=kernel_size, stride=stride)\n",
        "print(output.shape) # [B, C, H, W]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvtCDyPWWd4Q"
      },
      "outputs": [],
      "source": [
        "#Example of 1 image that is patched together (used in the poster)\n",
        "import matplotlib.pylab as pylab\n",
        "params = {'legend.fontsize': 'x-large',\n",
        "          'figure.figsize': (15, 5),\n",
        "         'axes.labelsize': 'x-large',\n",
        "         'axes.titlesize':'x-large',\n",
        "         'xtick.labelsize':'x-large',\n",
        "         'ytick.labelsize':'x-large'}\n",
        "pylab.rcParams.update(params)\n",
        "\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(output[0,0,:,:].detach().cpu())\n",
        "plt.colorbar(ticks=[0,1,2],label='Label number')\n",
        "plt.title('Predicted labels')\n",
        "plt.xlabel('Number of pixels [#]')\n",
        "plt.ylabel('Number of pixels [#]')\n",
        "\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(targets_val[0,:,:].detach().cpu())\n",
        "plt.colorbar(ticks=[0,1,2],label='Label number')\n",
        "plt.title('Target labels')\n",
        "plt.xlabel('Number of pixels [#]')\n",
        "plt.ylabel('Number of pixels [#]')\n",
        "\n",
        "plt.figure(dpi=200)\n",
        "plt.imshow(np.abs(targets_val[0,:,:].detach().cpu().numpy()-output[0,0,:,:].detach().cpu().numpy()))\n",
        "plt.colorbar(ticks=[0,1,2],label='Absolute label difference')\n",
        "plt.title('Prediction vs. targets')\n",
        "plt.xlabel('Number of pixels [#]')\n",
        "plt.ylabel('Number of pixels [#]')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmfpaxCY3mdN"
      },
      "outputs": [],
      "source": [
        "#Plots of training, validation and test accuracies and loss function\n",
        "import matplotlib.pylab as pylab\n",
        "params = {'legend.fontsize': 'x-large',\n",
        "          'figure.figsize': (15, 5),\n",
        "         'axes.labelsize': 'x-large',\n",
        "         'axes.titlesize':'x-large',\n",
        "         'xtick.labelsize':'x-large',\n",
        "         'ytick.labelsize':'x-large'}\n",
        "pylab.rcParams.update(params)\n",
        "\n",
        "fig,axs=plt.subplots(1,2,figsize=(15,5))\n",
        "\n",
        "axs[0].plot(train_accuracies_dice,color='red',label='Dice')\n",
        "axs[0].plot(train_accuracies_jaccard,color='blue',label='Jaccard')\n",
        "axs[0].plot(train_accuracies_pixel,color='green',label='Pixel-wise')\n",
        "axs[0].grid()\n",
        "axs[0].legend(loc='lower right')\n",
        "axs[0].set_title('Training accuracies')\n",
        "axs[0].set_xlabel('Epoch number')\n",
        "axs[0].set_ylabel('Accuracy')\n",
        "axs[0].set_ylim([0,1])\n",
        "\n",
        "axs[1].plot(valid_accuracies_dice,color='red',label='Dice')\n",
        "axs[1].plot(valid_accuracies_jaccard,color='blue',label='Jaccard')\n",
        "axs[1].plot(valid_accuracies_pixel,color='green',label='Pixel-wise')\n",
        "axs[1].legend(loc='lower right',fontsize=16)\n",
        "axs[1].tick_params(axis='both', which='minor', labelsize=16)\n",
        "axs[1].grid()\n",
        "axs[1].set_title('Validation accuracies',fontsize=20)\n",
        "axs[1].set_xlabel('Epoch number',fontsize=18)\n",
        "axs[1].set_ylabel('Accuracy',fontsize=18)\n",
        "axs[1].set_ylim([0,1])\n",
        "plt.tight_layout()\n",
        "\n",
        "params = {'legend.fontsize': 'x-large',\n",
        "          'figure.figsize': (15, 5),\n",
        "         'axes.labelsize': 'x-large',\n",
        "         'axes.titlesize':'x-large',\n",
        "         'xtick.labelsize':'x-large',\n",
        "         'ytick.labelsize':'x-large'}\n",
        "pylab.rcParams.update(params)\n",
        "\n",
        "fig,axs=plt.subplots(1,3,figsize=(15,5))\n",
        "axs[0].plot(train_accuracies_dice,color='red',label='Training accuracy')\n",
        "axs[0].plot(valid_accuracies_dice,color='blue',label='Validation accuracy')\n",
        "axs[0].set_title('Dice coefficient accuracies')\n",
        "axs[0].set_xlabel('Epoch number')\n",
        "axs[0].grid()\n",
        "axs[0].legend(loc='lower right')\n",
        "axs[0].set_ylabel('Accuracy')\n",
        "axs[0].set_ylim([0,1])\n",
        "\n",
        "axs[1].plot(train_accuracies_jaccard,color='red',label='Training accuracy')\n",
        "axs[1].plot(valid_accuracies_jaccard,color='blue',label='Validation accuracy')\n",
        "axs[1].set_title('Jaccard coefficient accuracies')\n",
        "axs[1].set_xlabel('Epoch number')\n",
        "axs[1].grid()\n",
        "axs[1].legend(loc='lower right')\n",
        "axs[1].set_ylabel('Accuracy')\n",
        "axs[1].set_ylim([0,1])\n",
        "\n",
        "axs[2].plot(train_accuracies_pixel,color='red',label='Training accuracy')\n",
        "axs[2].plot(valid_accuracies_pixel,color='blue',label='Validation accuracy')\n",
        "axs[2].set_title('Pixel-wise coefficient accuracies')\n",
        "axs[2].set_xlabel('Epoch number')\n",
        "axs[2].grid()\n",
        "axs[2].legend(loc='lower right')\n",
        "axs[2].set_ylabel('Accuracy')\n",
        "axs[2].set_ylim([0,1])\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "params = {'legend.fontsize': 'x-large',\n",
        "          'figure.figsize': (15, 5),\n",
        "         'axes.labelsize': 'x-large',\n",
        "         'axes.titlesize':'x-large',\n",
        "         'xtick.labelsize':'x-large',\n",
        "         'ytick.labelsize':'x-large'}\n",
        "pylab.rcParams.update(params)\n",
        "\n",
        "fig,axs=plt.subplots(1,1,figsize=(15,5))\n",
        "axs.plot(loss_train,color='blue',label='Training loss')\n",
        "axs.plot(val_loss,color='red',label='Validation loss')\n",
        "axs.legend()\n",
        "axs.grid()\n",
        "axs.set_title('Loss')\n",
        "axs.set_xlabel('Epoch number')\n",
        "axs.set_ylabel('Loss')\n",
        "axs.set_ylim([0,1])\n",
        "\n",
        "plt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}